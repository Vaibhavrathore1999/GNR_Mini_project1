{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/cyizhuo/CUB-200-2011-dataset.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYVk14M6hfW0",
        "outputId": "99071b96-ff63-4291-e7e2-cefe458dfa56"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CUB-200-2011-dataset' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "drZvFpdxRDgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inception-ResNet V2 in PyTorch\n",
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x, inplace=True)"
      ],
      "metadata": {
        "id": "cjYoXKo7gsCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionResNetV2(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(InceptionResNetV2, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            BasicConv2d(3, 32, kernel_size=3, stride=2),\n",
        "            BasicConv2d(32, 32, kernel_size=3),\n",
        "            BasicConv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            BasicConv2d(64, 80, kernel_size=1),\n",
        "            BasicConv2d(80, 192, kernel_size=3),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            InceptionResNetBlock(192, scale=0.17),\n",
        "            InceptionResNetBlock(192, scale=0.17),\n",
        "            InceptionResNetBlock(192, scale=0.17),\n",
        "            InceptionResNetBlock(192, scale=0.17),\n",
        "            InceptionResNetBlock(192, scale=0.17),\n",
        "        )\n",
        "\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(192, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.global_avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-mHy1fLEgxq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, scale=1.0):\n",
        "        super(InceptionResNetBlock, self).__init__()\n",
        "        self.branch0 = BasicConv2d(in_channels, 32, kernel_size=1)\n",
        "\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 32, kernel_size=1),\n",
        "            BasicConv2d(32, 32, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 32, kernel_size=1),\n",
        "            BasicConv2d(32, 48, kernel_size=3, padding=1),\n",
        "            BasicConv2d(48, 64, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.AvgPool2d(3, stride=1, padding=1),\n",
        "            BasicConv2d(in_channels, 64, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat([x0, x1, x2, x3], 1)\n",
        "        out = out * self.scale + x\n",
        "        return F.relu(out)\n"
      ],
      "metadata": {
        "id": "bbEUuQGjg-Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = InceptionResNetV2()"
      ],
      "metadata": {
        "id": "WD0MlRLehCUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "7M62InYmhJ3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trial 2 : InceptionV3"
      ],
      "metadata": {
        "id": "KEgXByiXtWyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.utils\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "9JRz8pR1tnGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "hSBYRSzOtpnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torchvision/models.html\n",
        "# https://github.com/pytorch/vision/tree/master/torchvision/models\n",
        "# 미리 사용할 모델의 Input 파악 필수!\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(299),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(), # ToTensor : [0, 255] -> [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(), # ToTensor : [0, 255] -> [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_data = dsets.ImageFolder('/content/CUB-200-2011-dataset/train', train_transform)\n",
        "test_data = dsets.ImageFolder('/content/CUB-200-2011-dataset/test', test_transform)"
      ],
      "metadata": {
        "id": "f2T8Nnw9tyC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_data,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True)"
      ],
      "metadata": {
        "id": "Wqi0JLYWt8H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img, title):\n",
        "    img = torchvision.utils.make_grid(img, normalize=True)\n",
        "    npimg = img.numpy()\n",
        "    fig = plt.figure(figsize = (5, 15))\n",
        "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DqwNq2CFt-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(train_loader)\n",
        "images,labels = next(dataiter)\n",
        "\n",
        "imshow(images, [train_data.classes[i] for i in labels])"
      ],
      "metadata": {
        "id": "Cr0ZHPL1uCEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.inception_v3(pretrained=True)"
      ],
      "metadata": {
        "id": "Kqj9WMS_ut5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters in the model: {total_params}\")"
      ],
      "metadata": {
        "id": "JYabb7rIu0pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "WRJ4VQ8nu6CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.aux_logits = False\n",
        "\n",
        "for parameter in model.parameters():\n",
        "    parameter.requires_grad = False"
      ],
      "metadata": {
        "id": "Y8PigrMbvPhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(model.fc.in_features, 200),\n",
        "    nn.Linear(200, 200)\n",
        ")"
      ],
      "metadata": {
        "id": "0cI2t2eUvUFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "RXpeaLtRvpMc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"
      ],
      "metadata": {
        "id": "qHtEToIYwEuq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    total_batch = len(train_data)//batch_size\n",
        "\n",
        "    for i, (batch_images, batch_labels) in enumerate(train_loader):\n",
        "\n",
        "        X = batch_images.to(device)\n",
        "        Y = batch_labels.to(device)\n",
        "\n",
        "        pre = model(X)\n",
        "        cost = loss(pre, Y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 5 == 0:\n",
        "            print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
        "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "luY-dgTx1JJ8",
        "outputId": "a458f52e-6c6f-4b91-c86a-900e615caf08"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], lter [5/1198] Loss: 10.6910\n",
            "Epoch [1/30], lter [10/1198] Loss: 7.8522\n",
            "Epoch [1/30], lter [15/1198] Loss: 5.3572\n",
            "Epoch [1/30], lter [20/1198] Loss: 5.6290\n",
            "Epoch [1/30], lter [25/1198] Loss: 5.5459\n",
            "Epoch [1/30], lter [30/1198] Loss: 5.4429\n",
            "Epoch [1/30], lter [35/1198] Loss: 5.9395\n",
            "Epoch [1/30], lter [40/1198] Loss: 6.3657\n",
            "Epoch [1/30], lter [45/1198] Loss: 5.2014\n",
            "Epoch [1/30], lter [50/1198] Loss: 5.5290\n",
            "Epoch [1/30], lter [55/1198] Loss: 5.6559\n",
            "Epoch [1/30], lter [60/1198] Loss: 5.9758\n",
            "Epoch [1/30], lter [65/1198] Loss: 5.7673\n",
            "Epoch [1/30], lter [70/1198] Loss: 5.9051\n",
            "Epoch [1/30], lter [75/1198] Loss: 5.4446\n",
            "Epoch [1/30], lter [80/1198] Loss: 5.7331\n",
            "Epoch [1/30], lter [85/1198] Loss: 6.2610\n",
            "Epoch [1/30], lter [90/1198] Loss: 5.4560\n",
            "Epoch [1/30], lter [95/1198] Loss: 5.4248\n",
            "Epoch [1/30], lter [100/1198] Loss: 5.4642\n",
            "Epoch [1/30], lter [105/1198] Loss: 5.8285\n",
            "Epoch [1/30], lter [110/1198] Loss: 5.3596\n",
            "Epoch [1/30], lter [115/1198] Loss: 5.0237\n",
            "Epoch [1/30], lter [120/1198] Loss: 5.5200\n",
            "Epoch [1/30], lter [125/1198] Loss: 5.8227\n",
            "Epoch [1/30], lter [130/1198] Loss: 5.7148\n",
            "Epoch [1/30], lter [135/1198] Loss: 5.6485\n",
            "Epoch [1/30], lter [140/1198] Loss: 5.7609\n",
            "Epoch [1/30], lter [145/1198] Loss: 5.2043\n",
            "Epoch [1/30], lter [150/1198] Loss: 5.0903\n",
            "Epoch [1/30], lter [155/1198] Loss: 4.9683\n",
            "Epoch [1/30], lter [160/1198] Loss: 5.8920\n",
            "Epoch [1/30], lter [165/1198] Loss: 5.8153\n",
            "Epoch [1/30], lter [170/1198] Loss: 4.7764\n",
            "Epoch [1/30], lter [175/1198] Loss: 5.4616\n",
            "Epoch [1/30], lter [180/1198] Loss: 5.3182\n",
            "Epoch [1/30], lter [185/1198] Loss: 5.9317\n",
            "Epoch [1/30], lter [190/1198] Loss: 4.7187\n",
            "Epoch [1/30], lter [195/1198] Loss: 5.4615\n",
            "Epoch [1/30], lter [200/1198] Loss: 5.5127\n",
            "Epoch [1/30], lter [205/1198] Loss: 4.8457\n",
            "Epoch [1/30], lter [210/1198] Loss: 5.7134\n",
            "Epoch [1/30], lter [215/1198] Loss: 5.6006\n",
            "Epoch [1/30], lter [220/1198] Loss: 4.5712\n",
            "Epoch [1/30], lter [225/1198] Loss: 5.7948\n",
            "Epoch [1/30], lter [230/1198] Loss: 5.6405\n",
            "Epoch [1/30], lter [235/1198] Loss: 4.6129\n",
            "Epoch [1/30], lter [240/1198] Loss: 4.9927\n",
            "Epoch [1/30], lter [245/1198] Loss: 5.5240\n",
            "Epoch [1/30], lter [250/1198] Loss: 5.1212\n",
            "Epoch [1/30], lter [255/1198] Loss: 4.7313\n",
            "Epoch [1/30], lter [260/1198] Loss: 5.1734\n",
            "Epoch [1/30], lter [265/1198] Loss: 4.8382\n",
            "Epoch [1/30], lter [270/1198] Loss: 5.1007\n",
            "Epoch [1/30], lter [275/1198] Loss: 5.2143\n",
            "Epoch [1/30], lter [280/1198] Loss: 4.9308\n",
            "Epoch [1/30], lter [285/1198] Loss: 5.6576\n",
            "Epoch [1/30], lter [290/1198] Loss: 5.6349\n",
            "Epoch [1/30], lter [295/1198] Loss: 5.4271\n",
            "Epoch [1/30], lter [300/1198] Loss: 5.8288\n",
            "Epoch [1/30], lter [305/1198] Loss: 4.6399\n",
            "Epoch [1/30], lter [310/1198] Loss: 5.2405\n",
            "Epoch [1/30], lter [315/1198] Loss: 3.8533\n",
            "Epoch [1/30], lter [320/1198] Loss: 5.4109\n",
            "Epoch [1/30], lter [325/1198] Loss: 4.5398\n",
            "Epoch [1/30], lter [330/1198] Loss: 4.9496\n",
            "Epoch [1/30], lter [335/1198] Loss: 5.8108\n",
            "Epoch [1/30], lter [340/1198] Loss: 6.0423\n",
            "Epoch [1/30], lter [345/1198] Loss: 4.6107\n",
            "Epoch [1/30], lter [350/1198] Loss: 4.3837\n",
            "Epoch [1/30], lter [355/1198] Loss: 4.2816\n",
            "Epoch [1/30], lter [360/1198] Loss: 5.1557\n",
            "Epoch [1/30], lter [365/1198] Loss: 5.0535\n",
            "Epoch [1/30], lter [370/1198] Loss: 5.2895\n",
            "Epoch [1/30], lter [375/1198] Loss: 5.1715\n",
            "Epoch [1/30], lter [380/1198] Loss: 4.3398\n",
            "Epoch [1/30], lter [385/1198] Loss: 4.3778\n",
            "Epoch [1/30], lter [390/1198] Loss: 4.1440\n",
            "Epoch [1/30], lter [395/1198] Loss: 3.8721\n",
            "Epoch [1/30], lter [400/1198] Loss: 4.6264\n",
            "Epoch [1/30], lter [405/1198] Loss: 3.9658\n",
            "Epoch [1/30], lter [410/1198] Loss: 6.1161\n",
            "Epoch [1/30], lter [415/1198] Loss: 5.1562\n",
            "Epoch [1/30], lter [420/1198] Loss: 5.2652\n",
            "Epoch [1/30], lter [425/1198] Loss: 4.2962\n",
            "Epoch [1/30], lter [430/1198] Loss: 4.0638\n",
            "Epoch [1/30], lter [435/1198] Loss: 5.3934\n",
            "Epoch [1/30], lter [440/1198] Loss: 5.7714\n",
            "Epoch [1/30], lter [445/1198] Loss: 5.0061\n",
            "Epoch [1/30], lter [450/1198] Loss: 4.6393\n",
            "Epoch [1/30], lter [455/1198] Loss: 4.7980\n",
            "Epoch [1/30], lter [460/1198] Loss: 5.0506\n",
            "Epoch [1/30], lter [465/1198] Loss: 5.2747\n",
            "Epoch [1/30], lter [470/1198] Loss: 4.9990\n",
            "Epoch [1/30], lter [475/1198] Loss: 4.4885\n",
            "Epoch [1/30], lter [480/1198] Loss: 4.7850\n",
            "Epoch [1/30], lter [485/1198] Loss: 4.6845\n",
            "Epoch [1/30], lter [490/1198] Loss: 4.8332\n",
            "Epoch [1/30], lter [495/1198] Loss: 4.8328\n",
            "Epoch [1/30], lter [500/1198] Loss: 4.9562\n",
            "Epoch [1/30], lter [505/1198] Loss: 4.4441\n",
            "Epoch [1/30], lter [510/1198] Loss: 5.5621\n",
            "Epoch [1/30], lter [515/1198] Loss: 5.3917\n",
            "Epoch [1/30], lter [520/1198] Loss: 4.5948\n",
            "Epoch [1/30], lter [525/1198] Loss: 5.8821\n",
            "Epoch [1/30], lter [530/1198] Loss: 4.8052\n",
            "Epoch [1/30], lter [535/1198] Loss: 4.5606\n",
            "Epoch [1/30], lter [540/1198] Loss: 4.7836\n",
            "Epoch [1/30], lter [545/1198] Loss: 5.4459\n",
            "Epoch [1/30], lter [550/1198] Loss: 3.6491\n",
            "Epoch [1/30], lter [555/1198] Loss: 5.3779\n",
            "Epoch [1/30], lter [560/1198] Loss: 4.9715\n",
            "Epoch [1/30], lter [565/1198] Loss: 4.9487\n",
            "Epoch [1/30], lter [570/1198] Loss: 4.2992\n",
            "Epoch [1/30], lter [575/1198] Loss: 4.6096\n",
            "Epoch [1/30], lter [580/1198] Loss: 4.1384\n",
            "Epoch [1/30], lter [585/1198] Loss: 4.1078\n",
            "Epoch [1/30], lter [590/1198] Loss: 5.3186\n",
            "Epoch [1/30], lter [595/1198] Loss: 4.2285\n",
            "Epoch [1/30], lter [600/1198] Loss: 4.5662\n",
            "Epoch [1/30], lter [605/1198] Loss: 5.1246\n",
            "Epoch [1/30], lter [610/1198] Loss: 4.3628\n",
            "Epoch [1/30], lter [615/1198] Loss: 4.3257\n",
            "Epoch [1/30], lter [620/1198] Loss: 4.7872\n",
            "Epoch [1/30], lter [625/1198] Loss: 4.9715\n",
            "Epoch [1/30], lter [630/1198] Loss: 5.1184\n",
            "Epoch [1/30], lter [635/1198] Loss: 4.8797\n",
            "Epoch [1/30], lter [640/1198] Loss: 4.9353\n",
            "Epoch [1/30], lter [645/1198] Loss: 4.8023\n",
            "Epoch [1/30], lter [650/1198] Loss: 4.9546\n",
            "Epoch [1/30], lter [655/1198] Loss: 4.5594\n",
            "Epoch [1/30], lter [660/1198] Loss: 3.9336\n",
            "Epoch [1/30], lter [665/1198] Loss: 4.3307\n",
            "Epoch [1/30], lter [670/1198] Loss: 4.8098\n",
            "Epoch [1/30], lter [675/1198] Loss: 5.4010\n",
            "Epoch [1/30], lter [680/1198] Loss: 3.5718\n",
            "Epoch [1/30], lter [685/1198] Loss: 4.6092\n",
            "Epoch [1/30], lter [690/1198] Loss: 4.0050\n",
            "Epoch [1/30], lter [695/1198] Loss: 4.4923\n",
            "Epoch [1/30], lter [700/1198] Loss: 4.9466\n",
            "Epoch [1/30], lter [705/1198] Loss: 2.8049\n",
            "Epoch [1/30], lter [710/1198] Loss: 4.6207\n",
            "Epoch [1/30], lter [715/1198] Loss: 4.1092\n",
            "Epoch [1/30], lter [720/1198] Loss: 3.5788\n",
            "Epoch [1/30], lter [725/1198] Loss: 4.6412\n",
            "Epoch [1/30], lter [730/1198] Loss: 3.9676\n",
            "Epoch [1/30], lter [735/1198] Loss: 4.5397\n",
            "Epoch [1/30], lter [740/1198] Loss: 4.8338\n",
            "Epoch [1/30], lter [745/1198] Loss: 4.2689\n",
            "Epoch [1/30], lter [750/1198] Loss: 4.3989\n",
            "Epoch [1/30], lter [755/1198] Loss: 4.7692\n",
            "Epoch [1/30], lter [760/1198] Loss: 4.1792\n",
            "Epoch [1/30], lter [765/1198] Loss: 4.3055\n",
            "Epoch [1/30], lter [770/1198] Loss: 4.9300\n",
            "Epoch [1/30], lter [775/1198] Loss: 3.8550\n",
            "Epoch [1/30], lter [780/1198] Loss: 3.9388\n",
            "Epoch [1/30], lter [785/1198] Loss: 3.7154\n",
            "Epoch [1/30], lter [790/1198] Loss: 4.5768\n",
            "Epoch [1/30], lter [795/1198] Loss: 4.5265\n",
            "Epoch [1/30], lter [800/1198] Loss: 3.7983\n",
            "Epoch [1/30], lter [805/1198] Loss: 3.5362\n",
            "Epoch [1/30], lter [810/1198] Loss: 4.6195\n",
            "Epoch [1/30], lter [815/1198] Loss: 5.0449\n",
            "Epoch [1/30], lter [820/1198] Loss: 4.5790\n",
            "Epoch [1/30], lter [825/1198] Loss: 4.1129\n",
            "Epoch [1/30], lter [830/1198] Loss: 4.7638\n",
            "Epoch [1/30], lter [835/1198] Loss: 5.0586\n",
            "Epoch [1/30], lter [840/1198] Loss: 4.7084\n",
            "Epoch [1/30], lter [845/1198] Loss: 4.7077\n",
            "Epoch [1/30], lter [850/1198] Loss: 3.5290\n",
            "Epoch [1/30], lter [855/1198] Loss: 4.4940\n",
            "Epoch [1/30], lter [860/1198] Loss: 4.9763\n",
            "Epoch [1/30], lter [865/1198] Loss: 4.4105\n",
            "Epoch [1/30], lter [870/1198] Loss: 4.6396\n",
            "Epoch [1/30], lter [875/1198] Loss: 4.1792\n",
            "Epoch [1/30], lter [880/1198] Loss: 4.2854\n",
            "Epoch [1/30], lter [885/1198] Loss: 4.7229\n",
            "Epoch [1/30], lter [890/1198] Loss: 4.2829\n",
            "Epoch [1/30], lter [895/1198] Loss: 4.2298\n",
            "Epoch [1/30], lter [900/1198] Loss: 4.7200\n",
            "Epoch [1/30], lter [905/1198] Loss: 4.3691\n",
            "Epoch [1/30], lter [910/1198] Loss: 4.4539\n",
            "Epoch [1/30], lter [915/1198] Loss: 4.9682\n",
            "Epoch [1/30], lter [920/1198] Loss: 3.7316\n",
            "Epoch [1/30], lter [925/1198] Loss: 4.9970\n",
            "Epoch [1/30], lter [930/1198] Loss: 4.7969\n",
            "Epoch [1/30], lter [935/1198] Loss: 5.0208\n",
            "Epoch [1/30], lter [940/1198] Loss: 5.0864\n",
            "Epoch [1/30], lter [945/1198] Loss: 4.2761\n",
            "Epoch [1/30], lter [950/1198] Loss: 4.4165\n",
            "Epoch [1/30], lter [955/1198] Loss: 4.3230\n",
            "Epoch [1/30], lter [960/1198] Loss: 3.8847\n",
            "Epoch [1/30], lter [965/1198] Loss: 4.3961\n",
            "Epoch [1/30], lter [970/1198] Loss: 4.3638\n",
            "Epoch [1/30], lter [975/1198] Loss: 5.2092\n",
            "Epoch [1/30], lter [980/1198] Loss: 4.2094\n",
            "Epoch [1/30], lter [985/1198] Loss: 4.7580\n",
            "Epoch [1/30], lter [990/1198] Loss: 4.8956\n",
            "Epoch [1/30], lter [995/1198] Loss: 4.1622\n",
            "Epoch [1/30], lter [1000/1198] Loss: 3.4625\n",
            "Epoch [1/30], lter [1005/1198] Loss: 3.2629\n",
            "Epoch [1/30], lter [1010/1198] Loss: 4.4420\n",
            "Epoch [1/30], lter [1015/1198] Loss: 3.9679\n",
            "Epoch [1/30], lter [1020/1198] Loss: 5.4443\n",
            "Epoch [1/30], lter [1025/1198] Loss: 3.7269\n",
            "Epoch [1/30], lter [1030/1198] Loss: 3.4677\n",
            "Epoch [1/30], lter [1035/1198] Loss: 5.1886\n",
            "Epoch [1/30], lter [1040/1198] Loss: 3.3324\n",
            "Epoch [1/30], lter [1045/1198] Loss: 5.1270\n",
            "Epoch [1/30], lter [1050/1198] Loss: 4.8613\n",
            "Epoch [1/30], lter [1055/1198] Loss: 3.5845\n",
            "Epoch [1/30], lter [1060/1198] Loss: 4.2393\n",
            "Epoch [1/30], lter [1065/1198] Loss: 5.3976\n",
            "Epoch [1/30], lter [1070/1198] Loss: 4.4401\n",
            "Epoch [1/30], lter [1075/1198] Loss: 3.6795\n",
            "Epoch [1/30], lter [1080/1198] Loss: 4.7362\n",
            "Epoch [1/30], lter [1085/1198] Loss: 4.8574\n",
            "Epoch [1/30], lter [1090/1198] Loss: 3.8004\n",
            "Epoch [1/30], lter [1095/1198] Loss: 4.7837\n",
            "Epoch [1/30], lter [1100/1198] Loss: 4.3345\n",
            "Epoch [1/30], lter [1105/1198] Loss: 6.2358\n",
            "Epoch [1/30], lter [1110/1198] Loss: 4.7877\n",
            "Epoch [1/30], lter [1115/1198] Loss: 3.9829\n",
            "Epoch [1/30], lter [1120/1198] Loss: 4.3342\n",
            "Epoch [1/30], lter [1125/1198] Loss: 3.2195\n",
            "Epoch [1/30], lter [1130/1198] Loss: 4.2833\n",
            "Epoch [1/30], lter [1135/1198] Loss: 4.0058\n",
            "Epoch [1/30], lter [1140/1198] Loss: 4.8538\n",
            "Epoch [1/30], lter [1145/1198] Loss: 3.9177\n",
            "Epoch [1/30], lter [1150/1198] Loss: 4.1260\n",
            "Epoch [1/30], lter [1155/1198] Loss: 4.8447\n",
            "Epoch [1/30], lter [1160/1198] Loss: 4.8596\n",
            "Epoch [1/30], lter [1165/1198] Loss: 4.7425\n",
            "Epoch [1/30], lter [1170/1198] Loss: 4.3998\n",
            "Epoch [1/30], lter [1175/1198] Loss: 5.2678\n",
            "Epoch [1/30], lter [1180/1198] Loss: 5.4851\n",
            "Epoch [1/30], lter [1185/1198] Loss: 5.3499\n",
            "Epoch [1/30], lter [1190/1198] Loss: 3.1901\n",
            "Epoch [1/30], lter [1195/1198] Loss: 3.6530\n",
            "Epoch [2/30], lter [5/1198] Loss: 3.4857\n",
            "Epoch [2/30], lter [10/1198] Loss: 4.3902\n",
            "Epoch [2/30], lter [15/1198] Loss: 2.6598\n",
            "Epoch [2/30], lter [20/1198] Loss: 4.4959\n",
            "Epoch [2/30], lter [25/1198] Loss: 4.0949\n",
            "Epoch [2/30], lter [30/1198] Loss: 3.9094\n",
            "Epoch [2/30], lter [35/1198] Loss: 3.7019\n",
            "Epoch [2/30], lter [40/1198] Loss: 5.6696\n",
            "Epoch [2/30], lter [45/1198] Loss: 4.0231\n",
            "Epoch [2/30], lter [50/1198] Loss: 4.5242\n",
            "Epoch [2/30], lter [55/1198] Loss: 3.5399\n",
            "Epoch [2/30], lter [60/1198] Loss: 3.6563\n",
            "Epoch [2/30], lter [65/1198] Loss: 3.1337\n",
            "Epoch [2/30], lter [70/1198] Loss: 3.6918\n",
            "Epoch [2/30], lter [75/1198] Loss: 3.0176\n",
            "Epoch [2/30], lter [80/1198] Loss: 3.6035\n",
            "Epoch [2/30], lter [85/1198] Loss: 4.7084\n",
            "Epoch [2/30], lter [90/1198] Loss: 5.5811\n",
            "Epoch [2/30], lter [95/1198] Loss: 4.8797\n",
            "Epoch [2/30], lter [100/1198] Loss: 4.0057\n",
            "Epoch [2/30], lter [105/1198] Loss: 3.6820\n",
            "Epoch [2/30], lter [110/1198] Loss: 3.2266\n",
            "Epoch [2/30], lter [115/1198] Loss: 4.5517\n",
            "Epoch [2/30], lter [120/1198] Loss: 3.3697\n",
            "Epoch [2/30], lter [125/1198] Loss: 4.2067\n",
            "Epoch [2/30], lter [130/1198] Loss: 3.0211\n",
            "Epoch [2/30], lter [135/1198] Loss: 4.7164\n",
            "Epoch [2/30], lter [140/1198] Loss: 4.9079\n",
            "Epoch [2/30], lter [145/1198] Loss: 3.8903\n",
            "Epoch [2/30], lter [150/1198] Loss: 4.1554\n",
            "Epoch [2/30], lter [155/1198] Loss: 3.4327\n",
            "Epoch [2/30], lter [160/1198] Loss: 4.8671\n",
            "Epoch [2/30], lter [165/1198] Loss: 4.1104\n",
            "Epoch [2/30], lter [170/1198] Loss: 4.7996\n",
            "Epoch [2/30], lter [175/1198] Loss: 3.1072\n",
            "Epoch [2/30], lter [180/1198] Loss: 2.8709\n",
            "Epoch [2/30], lter [185/1198] Loss: 4.4214\n",
            "Epoch [2/30], lter [190/1198] Loss: 4.8138\n",
            "Epoch [2/30], lter [195/1198] Loss: 2.2730\n",
            "Epoch [2/30], lter [200/1198] Loss: 3.1782\n",
            "Epoch [2/30], lter [205/1198] Loss: 3.7353\n",
            "Epoch [2/30], lter [210/1198] Loss: 3.4644\n",
            "Epoch [2/30], lter [215/1198] Loss: 4.4328\n",
            "Epoch [2/30], lter [220/1198] Loss: 4.5156\n",
            "Epoch [2/30], lter [225/1198] Loss: 3.4730\n",
            "Epoch [2/30], lter [230/1198] Loss: 4.3640\n",
            "Epoch [2/30], lter [235/1198] Loss: 3.6065\n",
            "Epoch [2/30], lter [240/1198] Loss: 4.4328\n",
            "Epoch [2/30], lter [245/1198] Loss: 3.5727\n",
            "Epoch [2/30], lter [250/1198] Loss: 3.9081\n",
            "Epoch [2/30], lter [255/1198] Loss: 4.4303\n",
            "Epoch [2/30], lter [260/1198] Loss: 4.0113\n",
            "Epoch [2/30], lter [265/1198] Loss: 3.4642\n",
            "Epoch [2/30], lter [270/1198] Loss: 3.6234\n",
            "Epoch [2/30], lter [275/1198] Loss: 4.6346\n",
            "Epoch [2/30], lter [280/1198] Loss: 2.6843\n",
            "Epoch [2/30], lter [285/1198] Loss: 5.4511\n",
            "Epoch [2/30], lter [290/1198] Loss: 4.3839\n",
            "Epoch [2/30], lter [295/1198] Loss: 4.1462\n",
            "Epoch [2/30], lter [300/1198] Loss: 4.0454\n",
            "Epoch [2/30], lter [305/1198] Loss: 4.4312\n",
            "Epoch [2/30], lter [310/1198] Loss: 3.6348\n",
            "Epoch [2/30], lter [315/1198] Loss: 4.6641\n",
            "Epoch [2/30], lter [320/1198] Loss: 4.6617\n",
            "Epoch [2/30], lter [325/1198] Loss: 4.5830\n",
            "Epoch [2/30], lter [330/1198] Loss: 2.4257\n",
            "Epoch [2/30], lter [335/1198] Loss: 3.5001\n",
            "Epoch [2/30], lter [340/1198] Loss: 5.7029\n",
            "Epoch [2/30], lter [345/1198] Loss: 3.9331\n",
            "Epoch [2/30], lter [350/1198] Loss: 4.5691\n",
            "Epoch [2/30], lter [355/1198] Loss: 3.0640\n",
            "Epoch [2/30], lter [360/1198] Loss: 3.9506\n",
            "Epoch [2/30], lter [365/1198] Loss: 4.6428\n",
            "Epoch [2/30], lter [370/1198] Loss: 4.3788\n",
            "Epoch [2/30], lter [375/1198] Loss: 1.8060\n",
            "Epoch [2/30], lter [380/1198] Loss: 3.1362\n",
            "Epoch [2/30], lter [385/1198] Loss: 3.3366\n",
            "Epoch [2/30], lter [390/1198] Loss: 3.2541\n",
            "Epoch [2/30], lter [395/1198] Loss: 4.2833\n",
            "Epoch [2/30], lter [400/1198] Loss: 4.5194\n",
            "Epoch [2/30], lter [405/1198] Loss: 4.2970\n",
            "Epoch [2/30], lter [410/1198] Loss: 4.8065\n",
            "Epoch [2/30], lter [415/1198] Loss: 4.5128\n",
            "Epoch [2/30], lter [420/1198] Loss: 3.9922\n",
            "Epoch [2/30], lter [425/1198] Loss: 3.8859\n",
            "Epoch [2/30], lter [430/1198] Loss: 3.8669\n",
            "Epoch [2/30], lter [435/1198] Loss: 4.3729\n",
            "Epoch [2/30], lter [440/1198] Loss: 3.8920\n",
            "Epoch [2/30], lter [445/1198] Loss: 3.2441\n",
            "Epoch [2/30], lter [450/1198] Loss: 3.4757\n",
            "Epoch [2/30], lter [455/1198] Loss: 5.1945\n",
            "Epoch [2/30], lter [460/1198] Loss: 3.7351\n",
            "Epoch [2/30], lter [465/1198] Loss: 5.0120\n",
            "Epoch [2/30], lter [470/1198] Loss: 4.1202\n",
            "Epoch [2/30], lter [475/1198] Loss: 3.8697\n",
            "Epoch [2/30], lter [480/1198] Loss: 4.3811\n",
            "Epoch [2/30], lter [485/1198] Loss: 3.4049\n",
            "Epoch [2/30], lter [490/1198] Loss: 3.1954\n",
            "Epoch [2/30], lter [495/1198] Loss: 4.9904\n",
            "Epoch [2/30], lter [500/1198] Loss: 4.7130\n",
            "Epoch [2/30], lter [505/1198] Loss: 4.9798\n",
            "Epoch [2/30], lter [510/1198] Loss: 3.4508\n",
            "Epoch [2/30], lter [515/1198] Loss: 4.2608\n",
            "Epoch [2/30], lter [520/1198] Loss: 4.6417\n",
            "Epoch [2/30], lter [525/1198] Loss: 4.0732\n",
            "Epoch [2/30], lter [530/1198] Loss: 3.5099\n",
            "Epoch [2/30], lter [535/1198] Loss: 5.1729\n",
            "Epoch [2/30], lter [540/1198] Loss: 4.3293\n",
            "Epoch [2/30], lter [545/1198] Loss: 4.0480\n",
            "Epoch [2/30], lter [550/1198] Loss: 4.1629\n",
            "Epoch [2/30], lter [555/1198] Loss: 5.1562\n",
            "Epoch [2/30], lter [560/1198] Loss: 2.7264\n",
            "Epoch [2/30], lter [565/1198] Loss: 4.9378\n",
            "Epoch [2/30], lter [570/1198] Loss: 3.7020\n",
            "Epoch [2/30], lter [575/1198] Loss: 2.3338\n",
            "Epoch [2/30], lter [580/1198] Loss: 4.7170\n",
            "Epoch [2/30], lter [585/1198] Loss: 3.3811\n",
            "Epoch [2/30], lter [590/1198] Loss: 3.8851\n",
            "Epoch [2/30], lter [595/1198] Loss: 3.3099\n",
            "Epoch [2/30], lter [600/1198] Loss: 2.6429\n",
            "Epoch [2/30], lter [605/1198] Loss: 5.2555\n",
            "Epoch [2/30], lter [610/1198] Loss: 3.8897\n",
            "Epoch [2/30], lter [615/1198] Loss: 2.7339\n",
            "Epoch [2/30], lter [620/1198] Loss: 4.4141\n",
            "Epoch [2/30], lter [625/1198] Loss: 3.2060\n",
            "Epoch [2/30], lter [630/1198] Loss: 4.4552\n",
            "Epoch [2/30], lter [635/1198] Loss: 4.2982\n",
            "Epoch [2/30], lter [640/1198] Loss: 3.6223\n",
            "Epoch [2/30], lter [645/1198] Loss: 5.2042\n",
            "Epoch [2/30], lter [650/1198] Loss: 5.1209\n",
            "Epoch [2/30], lter [655/1198] Loss: 3.8643\n",
            "Epoch [2/30], lter [660/1198] Loss: 3.8447\n",
            "Epoch [2/30], lter [665/1198] Loss: 3.5226\n",
            "Epoch [2/30], lter [670/1198] Loss: 3.0062\n",
            "Epoch [2/30], lter [675/1198] Loss: 4.6931\n",
            "Epoch [2/30], lter [680/1198] Loss: 2.7510\n",
            "Epoch [2/30], lter [685/1198] Loss: 3.8972\n",
            "Epoch [2/30], lter [690/1198] Loss: 3.5610\n",
            "Epoch [2/30], lter [695/1198] Loss: 3.1148\n",
            "Epoch [2/30], lter [700/1198] Loss: 5.5332\n",
            "Epoch [2/30], lter [705/1198] Loss: 2.3969\n",
            "Epoch [2/30], lter [710/1198] Loss: 5.2992\n",
            "Epoch [2/30], lter [715/1198] Loss: 3.1216\n",
            "Epoch [2/30], lter [720/1198] Loss: 5.1668\n",
            "Epoch [2/30], lter [725/1198] Loss: 3.8525\n",
            "Epoch [2/30], lter [730/1198] Loss: 3.6138\n",
            "Epoch [2/30], lter [735/1198] Loss: 4.1607\n",
            "Epoch [2/30], lter [740/1198] Loss: 5.0045\n",
            "Epoch [2/30], lter [745/1198] Loss: 4.4956\n",
            "Epoch [2/30], lter [750/1198] Loss: 2.0076\n",
            "Epoch [2/30], lter [755/1198] Loss: 4.6549\n",
            "Epoch [2/30], lter [760/1198] Loss: 4.6021\n",
            "Epoch [2/30], lter [765/1198] Loss: 4.2179\n",
            "Epoch [2/30], lter [770/1198] Loss: 2.4427\n",
            "Epoch [2/30], lter [775/1198] Loss: 3.6749\n",
            "Epoch [2/30], lter [780/1198] Loss: 4.5440\n",
            "Epoch [2/30], lter [785/1198] Loss: 3.9923\n",
            "Epoch [2/30], lter [790/1198] Loss: 4.9896\n",
            "Epoch [2/30], lter [795/1198] Loss: 4.1313\n",
            "Epoch [2/30], lter [800/1198] Loss: 3.6394\n",
            "Epoch [2/30], lter [805/1198] Loss: 3.8067\n",
            "Epoch [2/30], lter [810/1198] Loss: 3.5563\n",
            "Epoch [2/30], lter [815/1198] Loss: 4.1594\n",
            "Epoch [2/30], lter [820/1198] Loss: 4.3238\n",
            "Epoch [2/30], lter [825/1198] Loss: 2.4769\n",
            "Epoch [2/30], lter [830/1198] Loss: 4.1540\n",
            "Epoch [2/30], lter [835/1198] Loss: 4.8382\n",
            "Epoch [2/30], lter [840/1198] Loss: 2.3259\n",
            "Epoch [2/30], lter [845/1198] Loss: 4.2833\n",
            "Epoch [2/30], lter [850/1198] Loss: 3.4673\n",
            "Epoch [2/30], lter [855/1198] Loss: 3.9017\n",
            "Epoch [2/30], lter [860/1198] Loss: 3.8653\n",
            "Epoch [2/30], lter [865/1198] Loss: 3.4737\n",
            "Epoch [2/30], lter [870/1198] Loss: 3.7925\n",
            "Epoch [2/30], lter [875/1198] Loss: 3.5185\n",
            "Epoch [2/30], lter [880/1198] Loss: 3.8318\n",
            "Epoch [2/30], lter [885/1198] Loss: 4.2798\n",
            "Epoch [2/30], lter [890/1198] Loss: 2.5644\n",
            "Epoch [2/30], lter [895/1198] Loss: 3.8763\n",
            "Epoch [2/30], lter [900/1198] Loss: 3.7378\n",
            "Epoch [2/30], lter [905/1198] Loss: 3.8335\n",
            "Epoch [2/30], lter [910/1198] Loss: 4.0457\n",
            "Epoch [2/30], lter [915/1198] Loss: 3.8236\n",
            "Epoch [2/30], lter [920/1198] Loss: 4.2103\n",
            "Epoch [2/30], lter [925/1198] Loss: 2.9789\n",
            "Epoch [2/30], lter [930/1198] Loss: 3.5412\n",
            "Epoch [2/30], lter [935/1198] Loss: 4.4785\n",
            "Epoch [2/30], lter [940/1198] Loss: 3.7560\n",
            "Epoch [2/30], lter [945/1198] Loss: 3.0059\n",
            "Epoch [2/30], lter [950/1198] Loss: 4.0319\n",
            "Epoch [2/30], lter [955/1198] Loss: 3.0710\n",
            "Epoch [2/30], lter [960/1198] Loss: 4.4503\n",
            "Epoch [2/30], lter [965/1198] Loss: 1.8957\n",
            "Epoch [2/30], lter [970/1198] Loss: 4.0815\n",
            "Epoch [2/30], lter [975/1198] Loss: 2.4009\n",
            "Epoch [2/30], lter [980/1198] Loss: 3.9122\n",
            "Epoch [2/30], lter [985/1198] Loss: 2.8859\n",
            "Epoch [2/30], lter [990/1198] Loss: 3.6930\n",
            "Epoch [2/30], lter [995/1198] Loss: 3.7535\n",
            "Epoch [2/30], lter [1000/1198] Loss: 4.5098\n",
            "Epoch [2/30], lter [1005/1198] Loss: 3.6708\n",
            "Epoch [2/30], lter [1010/1198] Loss: 3.5406\n",
            "Epoch [2/30], lter [1015/1198] Loss: 3.1435\n",
            "Epoch [2/30], lter [1020/1198] Loss: 3.5015\n",
            "Epoch [2/30], lter [1025/1198] Loss: 4.5947\n",
            "Epoch [2/30], lter [1030/1198] Loss: 3.9665\n",
            "Epoch [2/30], lter [1035/1198] Loss: 5.0232\n",
            "Epoch [2/30], lter [1040/1198] Loss: 3.7285\n",
            "Epoch [2/30], lter [1045/1198] Loss: 2.5171\n",
            "Epoch [2/30], lter [1050/1198] Loss: 2.5984\n",
            "Epoch [2/30], lter [1055/1198] Loss: 3.2877\n",
            "Epoch [2/30], lter [1060/1198] Loss: 4.3205\n",
            "Epoch [2/30], lter [1065/1198] Loss: 4.3864\n",
            "Epoch [2/30], lter [1070/1198] Loss: 3.9208\n",
            "Epoch [2/30], lter [1075/1198] Loss: 4.8474\n",
            "Epoch [2/30], lter [1080/1198] Loss: 4.0278\n",
            "Epoch [2/30], lter [1085/1198] Loss: 3.8113\n",
            "Epoch [2/30], lter [1090/1198] Loss: 5.4088\n",
            "Epoch [2/30], lter [1095/1198] Loss: 4.0827\n",
            "Epoch [2/30], lter [1100/1198] Loss: 2.6491\n",
            "Epoch [2/30], lter [1105/1198] Loss: 2.9602\n",
            "Epoch [2/30], lter [1110/1198] Loss: 2.8402\n",
            "Epoch [2/30], lter [1115/1198] Loss: 2.2542\n",
            "Epoch [2/30], lter [1120/1198] Loss: 5.3406\n",
            "Epoch [2/30], lter [1125/1198] Loss: 3.2739\n",
            "Epoch [2/30], lter [1130/1198] Loss: 3.3397\n",
            "Epoch [2/30], lter [1135/1198] Loss: 4.0711\n",
            "Epoch [2/30], lter [1140/1198] Loss: 5.1326\n",
            "Epoch [2/30], lter [1145/1198] Loss: 4.7745\n",
            "Epoch [2/30], lter [1150/1198] Loss: 3.9277\n",
            "Epoch [2/30], lter [1155/1198] Loss: 2.9174\n",
            "Epoch [2/30], lter [1160/1198] Loss: 4.2987\n",
            "Epoch [2/30], lter [1165/1198] Loss: 2.7975\n",
            "Epoch [2/30], lter [1170/1198] Loss: 5.6021\n",
            "Epoch [2/30], lter [1175/1198] Loss: 4.4903\n",
            "Epoch [2/30], lter [1180/1198] Loss: 4.3405\n",
            "Epoch [2/30], lter [1185/1198] Loss: 4.3565\n",
            "Epoch [2/30], lter [1190/1198] Loss: 3.1982\n",
            "Epoch [2/30], lter [1195/1198] Loss: 3.4577\n",
            "Epoch [3/30], lter [5/1198] Loss: 3.9230\n",
            "Epoch [3/30], lter [10/1198] Loss: 5.4810\n",
            "Epoch [3/30], lter [15/1198] Loss: 4.1398\n",
            "Epoch [3/30], lter [20/1198] Loss: 2.7381\n",
            "Epoch [3/30], lter [25/1198] Loss: 3.4747\n",
            "Epoch [3/30], lter [30/1198] Loss: 4.6611\n",
            "Epoch [3/30], lter [35/1198] Loss: 3.7182\n",
            "Epoch [3/30], lter [40/1198] Loss: 4.5068\n",
            "Epoch [3/30], lter [45/1198] Loss: 4.8549\n",
            "Epoch [3/30], lter [50/1198] Loss: 4.1810\n",
            "Epoch [3/30], lter [55/1198] Loss: 2.4707\n",
            "Epoch [3/30], lter [60/1198] Loss: 5.7730\n",
            "Epoch [3/30], lter [65/1198] Loss: 2.4137\n",
            "Epoch [3/30], lter [70/1198] Loss: 3.1364\n",
            "Epoch [3/30], lter [75/1198] Loss: 3.0690\n",
            "Epoch [3/30], lter [80/1198] Loss: 2.7205\n",
            "Epoch [3/30], lter [85/1198] Loss: 2.0647\n",
            "Epoch [3/30], lter [90/1198] Loss: 2.9182\n",
            "Epoch [3/30], lter [95/1198] Loss: 5.3868\n",
            "Epoch [3/30], lter [100/1198] Loss: 4.8825\n",
            "Epoch [3/30], lter [105/1198] Loss: 4.9508\n",
            "Epoch [3/30], lter [110/1198] Loss: 4.0671\n",
            "Epoch [3/30], lter [115/1198] Loss: 3.2987\n",
            "Epoch [3/30], lter [120/1198] Loss: 4.3957\n",
            "Epoch [3/30], lter [125/1198] Loss: 2.5162\n",
            "Epoch [3/30], lter [130/1198] Loss: 3.3275\n",
            "Epoch [3/30], lter [135/1198] Loss: 4.5135\n",
            "Epoch [3/30], lter [140/1198] Loss: 3.2258\n",
            "Epoch [3/30], lter [145/1198] Loss: 4.1081\n",
            "Epoch [3/30], lter [150/1198] Loss: 3.7187\n",
            "Epoch [3/30], lter [155/1198] Loss: 4.0997\n",
            "Epoch [3/30], lter [160/1198] Loss: 4.8405\n",
            "Epoch [3/30], lter [165/1198] Loss: 5.2779\n",
            "Epoch [3/30], lter [170/1198] Loss: 4.5499\n",
            "Epoch [3/30], lter [175/1198] Loss: 3.6848\n",
            "Epoch [3/30], lter [180/1198] Loss: 4.4064\n",
            "Epoch [3/30], lter [185/1198] Loss: 3.6758\n",
            "Epoch [3/30], lter [190/1198] Loss: 5.1170\n",
            "Epoch [3/30], lter [195/1198] Loss: 4.0259\n",
            "Epoch [3/30], lter [200/1198] Loss: 2.8637\n",
            "Epoch [3/30], lter [205/1198] Loss: 3.1466\n",
            "Epoch [3/30], lter [210/1198] Loss: 3.1805\n",
            "Epoch [3/30], lter [215/1198] Loss: 2.5697\n",
            "Epoch [3/30], lter [220/1198] Loss: 3.4706\n",
            "Epoch [3/30], lter [225/1198] Loss: 3.7659\n",
            "Epoch [3/30], lter [230/1198] Loss: 3.9996\n",
            "Epoch [3/30], lter [235/1198] Loss: 3.4024\n",
            "Epoch [3/30], lter [240/1198] Loss: 4.3131\n",
            "Epoch [3/30], lter [245/1198] Loss: 5.0074\n",
            "Epoch [3/30], lter [250/1198] Loss: 4.0497\n",
            "Epoch [3/30], lter [255/1198] Loss: 4.1658\n",
            "Epoch [3/30], lter [260/1198] Loss: 2.8171\n",
            "Epoch [3/30], lter [265/1198] Loss: 4.5585\n",
            "Epoch [3/30], lter [270/1198] Loss: 4.2502\n",
            "Epoch [3/30], lter [275/1198] Loss: 5.0002\n",
            "Epoch [3/30], lter [280/1198] Loss: 4.2256\n",
            "Epoch [3/30], lter [285/1198] Loss: 4.7733\n",
            "Epoch [3/30], lter [290/1198] Loss: 3.5794\n",
            "Epoch [3/30], lter [295/1198] Loss: 4.4486\n",
            "Epoch [3/30], lter [300/1198] Loss: 2.3037\n",
            "Epoch [3/30], lter [305/1198] Loss: 2.1758\n",
            "Epoch [3/30], lter [310/1198] Loss: 4.0404\n",
            "Epoch [3/30], lter [315/1198] Loss: 3.6043\n",
            "Epoch [3/30], lter [320/1198] Loss: 4.2362\n",
            "Epoch [3/30], lter [325/1198] Loss: 2.5205\n",
            "Epoch [3/30], lter [330/1198] Loss: 2.5289\n",
            "Epoch [3/30], lter [335/1198] Loss: 4.3807\n",
            "Epoch [3/30], lter [340/1198] Loss: 4.3226\n",
            "Epoch [3/30], lter [345/1198] Loss: 5.2294\n",
            "Epoch [3/30], lter [350/1198] Loss: 3.7081\n",
            "Epoch [3/30], lter [355/1198] Loss: 2.9231\n",
            "Epoch [3/30], lter [360/1198] Loss: 3.0278\n",
            "Epoch [3/30], lter [365/1198] Loss: 4.2728\n",
            "Epoch [3/30], lter [370/1198] Loss: 3.5511\n",
            "Epoch [3/30], lter [375/1198] Loss: 3.0897\n",
            "Epoch [3/30], lter [380/1198] Loss: 5.7706\n",
            "Epoch [3/30], lter [385/1198] Loss: 4.0491\n",
            "Epoch [3/30], lter [390/1198] Loss: 4.0981\n",
            "Epoch [3/30], lter [395/1198] Loss: 5.1640\n",
            "Epoch [3/30], lter [400/1198] Loss: 4.1194\n",
            "Epoch [3/30], lter [405/1198] Loss: 3.6211\n",
            "Epoch [3/30], lter [410/1198] Loss: 4.6165\n",
            "Epoch [3/30], lter [415/1198] Loss: 4.9455\n",
            "Epoch [3/30], lter [420/1198] Loss: 4.6566\n",
            "Epoch [3/30], lter [425/1198] Loss: 2.6768\n",
            "Epoch [3/30], lter [430/1198] Loss: 2.7297\n",
            "Epoch [3/30], lter [435/1198] Loss: 3.8498\n",
            "Epoch [3/30], lter [440/1198] Loss: 3.8788\n",
            "Epoch [3/30], lter [445/1198] Loss: 5.2328\n",
            "Epoch [3/30], lter [450/1198] Loss: 3.3769\n",
            "Epoch [3/30], lter [455/1198] Loss: 3.2755\n",
            "Epoch [3/30], lter [460/1198] Loss: 3.2034\n",
            "Epoch [3/30], lter [465/1198] Loss: 5.0580\n",
            "Epoch [3/30], lter [470/1198] Loss: 4.8333\n",
            "Epoch [3/30], lter [475/1198] Loss: 3.7430\n",
            "Epoch [3/30], lter [480/1198] Loss: 3.8747\n",
            "Epoch [3/30], lter [485/1198] Loss: 4.0957\n",
            "Epoch [3/30], lter [490/1198] Loss: 3.4490\n",
            "Epoch [3/30], lter [495/1198] Loss: 3.2495\n",
            "Epoch [3/30], lter [500/1198] Loss: 3.7720\n",
            "Epoch [3/30], lter [505/1198] Loss: 3.8919\n",
            "Epoch [3/30], lter [510/1198] Loss: 4.2791\n",
            "Epoch [3/30], lter [515/1198] Loss: 4.6584\n",
            "Epoch [3/30], lter [520/1198] Loss: 4.2940\n",
            "Epoch [3/30], lter [525/1198] Loss: 4.7500\n",
            "Epoch [3/30], lter [530/1198] Loss: 2.8040\n",
            "Epoch [3/30], lter [535/1198] Loss: 3.3352\n",
            "Epoch [3/30], lter [540/1198] Loss: 3.0420\n",
            "Epoch [3/30], lter [545/1198] Loss: 3.1631\n",
            "Epoch [3/30], lter [550/1198] Loss: 5.0451\n",
            "Epoch [3/30], lter [555/1198] Loss: 4.7667\n",
            "Epoch [3/30], lter [560/1198] Loss: 3.7241\n",
            "Epoch [3/30], lter [565/1198] Loss: 4.5051\n",
            "Epoch [3/30], lter [570/1198] Loss: 3.5933\n",
            "Epoch [3/30], lter [575/1198] Loss: 2.8738\n",
            "Epoch [3/30], lter [580/1198] Loss: 2.9703\n",
            "Epoch [3/30], lter [585/1198] Loss: 4.4867\n",
            "Epoch [3/30], lter [590/1198] Loss: 3.6076\n",
            "Epoch [3/30], lter [595/1198] Loss: 3.3060\n",
            "Epoch [3/30], lter [600/1198] Loss: 4.2609\n",
            "Epoch [3/30], lter [605/1198] Loss: 2.7132\n",
            "Epoch [3/30], lter [610/1198] Loss: 2.9742\n",
            "Epoch [3/30], lter [615/1198] Loss: 3.6167\n",
            "Epoch [3/30], lter [620/1198] Loss: 2.9647\n",
            "Epoch [3/30], lter [625/1198] Loss: 3.8998\n",
            "Epoch [3/30], lter [630/1198] Loss: 2.2732\n",
            "Epoch [3/30], lter [635/1198] Loss: 3.7055\n",
            "Epoch [3/30], lter [640/1198] Loss: 3.8977\n",
            "Epoch [3/30], lter [645/1198] Loss: 3.8961\n",
            "Epoch [3/30], lter [650/1198] Loss: 2.5842\n",
            "Epoch [3/30], lter [655/1198] Loss: 3.7114\n",
            "Epoch [3/30], lter [660/1198] Loss: 3.6770\n",
            "Epoch [3/30], lter [665/1198] Loss: 2.5647\n",
            "Epoch [3/30], lter [670/1198] Loss: 4.2196\n",
            "Epoch [3/30], lter [675/1198] Loss: 4.3268\n",
            "Epoch [3/30], lter [680/1198] Loss: 3.6248\n",
            "Epoch [3/30], lter [685/1198] Loss: 2.6867\n",
            "Epoch [3/30], lter [690/1198] Loss: 4.3514\n",
            "Epoch [3/30], lter [695/1198] Loss: 3.7846\n",
            "Epoch [3/30], lter [700/1198] Loss: 4.2430\n",
            "Epoch [3/30], lter [705/1198] Loss: 4.6487\n",
            "Epoch [3/30], lter [710/1198] Loss: 4.0056\n",
            "Epoch [3/30], lter [715/1198] Loss: 3.7888\n",
            "Epoch [3/30], lter [720/1198] Loss: 4.4311\n",
            "Epoch [3/30], lter [725/1198] Loss: 4.2036\n",
            "Epoch [3/30], lter [730/1198] Loss: 4.0305\n",
            "Epoch [3/30], lter [735/1198] Loss: 3.7999\n",
            "Epoch [3/30], lter [740/1198] Loss: 4.1829\n",
            "Epoch [3/30], lter [745/1198] Loss: 3.3741\n",
            "Epoch [3/30], lter [750/1198] Loss: 5.9055\n",
            "Epoch [3/30], lter [755/1198] Loss: 4.5779\n",
            "Epoch [3/30], lter [760/1198] Loss: 2.8556\n",
            "Epoch [3/30], lter [765/1198] Loss: 2.8210\n",
            "Epoch [3/30], lter [770/1198] Loss: 3.1199\n",
            "Epoch [3/30], lter [775/1198] Loss: 3.0166\n",
            "Epoch [3/30], lter [780/1198] Loss: 4.1968\n",
            "Epoch [3/30], lter [785/1198] Loss: 2.6616\n",
            "Epoch [3/30], lter [790/1198] Loss: 4.1997\n",
            "Epoch [3/30], lter [795/1198] Loss: 2.7721\n",
            "Epoch [3/30], lter [800/1198] Loss: 2.2846\n",
            "Epoch [3/30], lter [805/1198] Loss: 4.7788\n",
            "Epoch [3/30], lter [810/1198] Loss: 2.3817\n",
            "Epoch [3/30], lter [815/1198] Loss: 5.0726\n",
            "Epoch [3/30], lter [820/1198] Loss: 4.2520\n",
            "Epoch [3/30], lter [825/1198] Loss: 4.5776\n",
            "Epoch [3/30], lter [830/1198] Loss: 3.6579\n",
            "Epoch [3/30], lter [835/1198] Loss: 2.9066\n",
            "Epoch [3/30], lter [840/1198] Loss: 2.3548\n",
            "Epoch [3/30], lter [845/1198] Loss: 2.8148\n",
            "Epoch [3/30], lter [850/1198] Loss: 2.3614\n",
            "Epoch [3/30], lter [855/1198] Loss: 2.2481\n",
            "Epoch [3/30], lter [860/1198] Loss: 3.7760\n",
            "Epoch [3/30], lter [865/1198] Loss: 3.2820\n",
            "Epoch [3/30], lter [870/1198] Loss: 2.8864\n",
            "Epoch [3/30], lter [875/1198] Loss: 2.5673\n",
            "Epoch [3/30], lter [880/1198] Loss: 5.6567\n",
            "Epoch [3/30], lter [885/1198] Loss: 4.1857\n",
            "Epoch [3/30], lter [890/1198] Loss: 4.2078\n",
            "Epoch [3/30], lter [895/1198] Loss: 3.6898\n",
            "Epoch [3/30], lter [900/1198] Loss: 3.2458\n",
            "Epoch [3/30], lter [905/1198] Loss: 3.3469\n",
            "Epoch [3/30], lter [910/1198] Loss: 3.9661\n",
            "Epoch [3/30], lter [915/1198] Loss: 4.1023\n",
            "Epoch [3/30], lter [920/1198] Loss: 4.9162\n",
            "Epoch [3/30], lter [925/1198] Loss: 4.8097\n",
            "Epoch [3/30], lter [930/1198] Loss: 3.1462\n",
            "Epoch [3/30], lter [935/1198] Loss: 4.8768\n",
            "Epoch [3/30], lter [940/1198] Loss: 4.1369\n",
            "Epoch [3/30], lter [945/1198] Loss: 4.3741\n",
            "Epoch [3/30], lter [950/1198] Loss: 3.0047\n",
            "Epoch [3/30], lter [955/1198] Loss: 3.0056\n",
            "Epoch [3/30], lter [960/1198] Loss: 5.3905\n",
            "Epoch [3/30], lter [965/1198] Loss: 3.3860\n",
            "Epoch [3/30], lter [970/1198] Loss: 3.0147\n",
            "Epoch [3/30], lter [975/1198] Loss: 2.9379\n",
            "Epoch [3/30], lter [980/1198] Loss: 2.5857\n",
            "Epoch [3/30], lter [985/1198] Loss: 3.3963\n",
            "Epoch [3/30], lter [990/1198] Loss: 3.5642\n",
            "Epoch [3/30], lter [995/1198] Loss: 3.0559\n",
            "Epoch [3/30], lter [1000/1198] Loss: 3.6181\n",
            "Epoch [3/30], lter [1005/1198] Loss: 4.0254\n",
            "Epoch [3/30], lter [1010/1198] Loss: 4.4135\n",
            "Epoch [3/30], lter [1015/1198] Loss: 2.9756\n",
            "Epoch [3/30], lter [1020/1198] Loss: 2.4460\n",
            "Epoch [3/30], lter [1025/1198] Loss: 4.0826\n",
            "Epoch [3/30], lter [1030/1198] Loss: 4.0457\n",
            "Epoch [3/30], lter [1035/1198] Loss: 5.8416\n",
            "Epoch [3/30], lter [1040/1198] Loss: 3.5577\n",
            "Epoch [3/30], lter [1045/1198] Loss: 3.3122\n",
            "Epoch [3/30], lter [1050/1198] Loss: 3.2706\n",
            "Epoch [3/30], lter [1055/1198] Loss: 2.1086\n",
            "Epoch [3/30], lter [1060/1198] Loss: 3.5048\n",
            "Epoch [3/30], lter [1065/1198] Loss: 4.7961\n",
            "Epoch [3/30], lter [1070/1198] Loss: 4.5758\n",
            "Epoch [3/30], lter [1075/1198] Loss: 2.5604\n",
            "Epoch [3/30], lter [1080/1198] Loss: 4.3511\n",
            "Epoch [3/30], lter [1085/1198] Loss: 4.2806\n",
            "Epoch [3/30], lter [1090/1198] Loss: 3.9131\n",
            "Epoch [3/30], lter [1095/1198] Loss: 2.8318\n",
            "Epoch [3/30], lter [1100/1198] Loss: 4.4454\n",
            "Epoch [3/30], lter [1105/1198] Loss: 3.4207\n",
            "Epoch [3/30], lter [1110/1198] Loss: 3.8738\n",
            "Epoch [3/30], lter [1115/1198] Loss: 3.6573\n",
            "Epoch [3/30], lter [1120/1198] Loss: 3.4795\n",
            "Epoch [3/30], lter [1125/1198] Loss: 2.6888\n",
            "Epoch [3/30], lter [1130/1198] Loss: 3.2004\n",
            "Epoch [3/30], lter [1135/1198] Loss: 3.3552\n",
            "Epoch [3/30], lter [1140/1198] Loss: 5.4767\n",
            "Epoch [3/30], lter [1145/1198] Loss: 4.6051\n",
            "Epoch [3/30], lter [1150/1198] Loss: 4.2289\n",
            "Epoch [3/30], lter [1155/1198] Loss: 2.5946\n",
            "Epoch [3/30], lter [1160/1198] Loss: 4.9047\n",
            "Epoch [3/30], lter [1165/1198] Loss: 2.6354\n",
            "Epoch [3/30], lter [1170/1198] Loss: 3.1152\n",
            "Epoch [3/30], lter [1175/1198] Loss: 3.8844\n",
            "Epoch [3/30], lter [1180/1198] Loss: 2.8631\n",
            "Epoch [3/30], lter [1185/1198] Loss: 5.2687\n",
            "Epoch [3/30], lter [1190/1198] Loss: 2.5473\n",
            "Epoch [3/30], lter [1195/1198] Loss: 4.1811\n",
            "Epoch [4/30], lter [5/1198] Loss: 4.1201\n",
            "Epoch [4/30], lter [10/1198] Loss: 3.4010\n",
            "Epoch [4/30], lter [15/1198] Loss: 3.1623\n",
            "Epoch [4/30], lter [20/1198] Loss: 4.4129\n",
            "Epoch [4/30], lter [25/1198] Loss: 2.6992\n",
            "Epoch [4/30], lter [30/1198] Loss: 3.7012\n",
            "Epoch [4/30], lter [35/1198] Loss: 5.2942\n",
            "Epoch [4/30], lter [40/1198] Loss: 3.7126\n",
            "Epoch [4/30], lter [45/1198] Loss: 2.3201\n",
            "Epoch [4/30], lter [50/1198] Loss: 4.3780\n",
            "Epoch [4/30], lter [55/1198] Loss: 4.1093\n",
            "Epoch [4/30], lter [60/1198] Loss: 4.3315\n",
            "Epoch [4/30], lter [65/1198] Loss: 1.6496\n",
            "Epoch [4/30], lter [70/1198] Loss: 4.4621\n",
            "Epoch [4/30], lter [75/1198] Loss: 4.8645\n",
            "Epoch [4/30], lter [80/1198] Loss: 4.6593\n",
            "Epoch [4/30], lter [85/1198] Loss: 5.2615\n",
            "Epoch [4/30], lter [90/1198] Loss: 2.0311\n",
            "Epoch [4/30], lter [95/1198] Loss: 2.3613\n",
            "Epoch [4/30], lter [100/1198] Loss: 4.5131\n",
            "Epoch [4/30], lter [105/1198] Loss: 3.4016\n",
            "Epoch [4/30], lter [110/1198] Loss: 3.2599\n",
            "Epoch [4/30], lter [115/1198] Loss: 2.7558\n",
            "Epoch [4/30], lter [120/1198] Loss: 3.8920\n",
            "Epoch [4/30], lter [125/1198] Loss: 3.1665\n",
            "Epoch [4/30], lter [130/1198] Loss: 3.3772\n",
            "Epoch [4/30], lter [135/1198] Loss: 3.4706\n",
            "Epoch [4/30], lter [140/1198] Loss: 3.5611\n",
            "Epoch [4/30], lter [145/1198] Loss: 2.8298\n",
            "Epoch [4/30], lter [150/1198] Loss: 4.9613\n",
            "Epoch [4/30], lter [155/1198] Loss: 3.7673\n",
            "Epoch [4/30], lter [160/1198] Loss: 3.1420\n",
            "Epoch [4/30], lter [165/1198] Loss: 4.7556\n",
            "Epoch [4/30], lter [170/1198] Loss: 3.7762\n",
            "Epoch [4/30], lter [175/1198] Loss: 3.7502\n",
            "Epoch [4/30], lter [180/1198] Loss: 4.1598\n",
            "Epoch [4/30], lter [185/1198] Loss: 5.7258\n",
            "Epoch [4/30], lter [190/1198] Loss: 4.2158\n",
            "Epoch [4/30], lter [195/1198] Loss: 3.1246\n",
            "Epoch [4/30], lter [200/1198] Loss: 2.5855\n",
            "Epoch [4/30], lter [205/1198] Loss: 2.1803\n",
            "Epoch [4/30], lter [210/1198] Loss: 3.8922\n",
            "Epoch [4/30], lter [215/1198] Loss: 5.2650\n",
            "Epoch [4/30], lter [220/1198] Loss: 5.0640\n",
            "Epoch [4/30], lter [225/1198] Loss: 2.8608\n",
            "Epoch [4/30], lter [230/1198] Loss: 2.7371\n",
            "Epoch [4/30], lter [235/1198] Loss: 4.9630\n",
            "Epoch [4/30], lter [240/1198] Loss: 3.1999\n",
            "Epoch [4/30], lter [245/1198] Loss: 2.9661\n",
            "Epoch [4/30], lter [250/1198] Loss: 4.1514\n",
            "Epoch [4/30], lter [255/1198] Loss: 3.3130\n",
            "Epoch [4/30], lter [260/1198] Loss: 4.3417\n",
            "Epoch [4/30], lter [265/1198] Loss: 2.6384\n",
            "Epoch [4/30], lter [270/1198] Loss: 3.9378\n",
            "Epoch [4/30], lter [275/1198] Loss: 3.4872\n",
            "Epoch [4/30], lter [280/1198] Loss: 2.1435\n",
            "Epoch [4/30], lter [285/1198] Loss: 2.7414\n",
            "Epoch [4/30], lter [290/1198] Loss: 5.6311\n",
            "Epoch [4/30], lter [295/1198] Loss: 4.3542\n",
            "Epoch [4/30], lter [300/1198] Loss: 4.6661\n",
            "Epoch [4/30], lter [305/1198] Loss: 3.1876\n",
            "Epoch [4/30], lter [310/1198] Loss: 4.1875\n",
            "Epoch [4/30], lter [315/1198] Loss: 3.4499\n",
            "Epoch [4/30], lter [320/1198] Loss: 5.0026\n",
            "Epoch [4/30], lter [325/1198] Loss: 4.8254\n",
            "Epoch [4/30], lter [330/1198] Loss: 4.3863\n",
            "Epoch [4/30], lter [335/1198] Loss: 2.5707\n",
            "Epoch [4/30], lter [340/1198] Loss: 4.3461\n",
            "Epoch [4/30], lter [345/1198] Loss: 2.6974\n",
            "Epoch [4/30], lter [350/1198] Loss: 3.2941\n",
            "Epoch [4/30], lter [355/1198] Loss: 4.0837\n",
            "Epoch [4/30], lter [360/1198] Loss: 3.9074\n",
            "Epoch [4/30], lter [365/1198] Loss: 3.7148\n",
            "Epoch [4/30], lter [370/1198] Loss: 3.4426\n",
            "Epoch [4/30], lter [375/1198] Loss: 4.6512\n",
            "Epoch [4/30], lter [380/1198] Loss: 2.2198\n",
            "Epoch [4/30], lter [385/1198] Loss: 1.5018\n",
            "Epoch [4/30], lter [390/1198] Loss: 2.9153\n",
            "Epoch [4/30], lter [395/1198] Loss: 3.0395\n",
            "Epoch [4/30], lter [400/1198] Loss: 3.1540\n",
            "Epoch [4/30], lter [405/1198] Loss: 3.2641\n",
            "Epoch [4/30], lter [410/1198] Loss: 3.2752\n",
            "Epoch [4/30], lter [415/1198] Loss: 2.5350\n",
            "Epoch [4/30], lter [420/1198] Loss: 4.7382\n",
            "Epoch [4/30], lter [425/1198] Loss: 2.3333\n",
            "Epoch [4/30], lter [430/1198] Loss: 6.2201\n",
            "Epoch [4/30], lter [435/1198] Loss: 4.1848\n",
            "Epoch [4/30], lter [440/1198] Loss: 4.2626\n",
            "Epoch [4/30], lter [445/1198] Loss: 2.9478\n",
            "Epoch [4/30], lter [450/1198] Loss: 3.1413\n",
            "Epoch [4/30], lter [455/1198] Loss: 3.8911\n",
            "Epoch [4/30], lter [460/1198] Loss: 3.3292\n",
            "Epoch [4/30], lter [465/1198] Loss: 4.6785\n",
            "Epoch [4/30], lter [470/1198] Loss: 3.8349\n",
            "Epoch [4/30], lter [475/1198] Loss: 3.0221\n",
            "Epoch [4/30], lter [480/1198] Loss: 4.3064\n",
            "Epoch [4/30], lter [485/1198] Loss: 4.0583\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-96ebf191b6c0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInceptionOutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0maux_defined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d_3b_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# N x 80 x 73 x 73\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d_4a_3x3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# N x 192 x 71 x 71\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for images, labels in test_loader:\n",
        "\n",
        "    images = images.to(device)\n",
        "    outputs = model(images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels.to(device)).sum()\n",
        "\n",
        "print('Accuracy of test images: %f %%' % (100 * float(correct) / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-PpJsID2UGP",
        "outputId": "9ef352b1-89b2-41c9-afd2-1f1ed4c2d360"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of test images: 31.532620 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"Black_footed_Albatross\", \"Laysan_Albatross\"]"
      ],
      "metadata": {
        "id": "q5A_3flnPYOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = iter(test_loader).next()\n",
        "\n",
        "outputs = model(images.to(device))\n",
        "\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(5)))\n",
        "\n",
        "title = (' '.join('%5s' % classes[labels[j]] for j in range(5)))\n",
        "imshow(torchvision.utils.make_grid(images, normalize=True), title)"
      ],
      "metadata": {
        "id": "2348CXq3Pj5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}